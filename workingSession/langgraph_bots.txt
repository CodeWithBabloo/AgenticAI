# First, install the necessary packages
!pip install --upgrade langchain langchain-community langgraph
!pip install langchain-ollama
!pip install langchain-groq

from typing import List, Dict
from langgraph.graph import StateGraph, START, END
from langchain_ollama.llms import OllamaLLM

# Step 1: Define State
class State(Dict):
    messages: List[Dict[str, str]] 

# Step 2: Initialize StateGraph
graph_builder = StateGraph(State)

# Initialize the LLM
#llm = OllamaLLM(model="llama3.1")
from langchain_groq import ChatGroq
llm = ChatGroq(api_key='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', model='llama-3.1-8b-instant')

# Define chatbot function
def chatbot(state: State):
    response = llm.invoke(state["messages"])
    state["messages"].append({"role": "assistant", "content": response})  # Treat response as a string
    return {"messages": state["messages"]}

# Add nodes and edges
graph_builder.add_node("chatbot", chatbot)
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)

# Compile the graph
graph = graph_builder.compile()

# Stream updates
def stream_graph_updates(user_input: str):    
    state = {"messages": [{"role": "user", "content": user_input}]}
    for event in graph.stream(state):
        for value in event.values():
            print("Assistant:", value["messages"][-1]["content"])

# Run chatbot in a loop
def run_chatbot():
    while True:
        try:
            # Using input from the user through the notebook interface
            user_input = input("Type your message (or 'quit' to exit): ")
            if user_input.lower() in ["quit", "exit", "q"]:
                print("Goodbye!")
                break

            stream_graph_updates(user_input)
        except Exception as e:
            print(f"An error occurred: {e}")
            break

# Call the function to start the chatbot if running in the notebook
run_chatbot()
